# -*- coding: utf-8 -*-
"""Exploratory Data Analysis Google apps

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12lAw9zVTiAlumDmwhXs4iWf0m5A2H1qH
"""

pip install tensorflow

pip install scikit-learn

#Importing the necesscary Libraries for analysis
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split as ts
import tensorflow as tf
import matplotlib.pyplot as plt
from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from tensorflow.python.data import Dataset
from tensorflow import keras

#Importing the dataset into a pandas Dataframe
gplay_df = pd.read_csv('/googleplaystore.csv')

#Displaying the head of the dataset
gplay_df.head()

"""## Data Cleaning"""

#Displaying the total number off Apps:
x = len(gplay_df)
print ('The number of records in the data set is: ', x)

#Removing duplicate Apps and filling Null values from the dataset:
gplay_df.drop_duplicates(subset='App', inplace=True)
gplay_df = gplay_df.fillna(0) #Eliminating NA values and replacing with 0
y = len(gplay_df)
print ('The number of unique App records in the data set is: ', y)

# Preparation for column Size
gplay_df['Size'] = gplay_df['Size'].apply(lambda a: str(a).replace('M', '') if 'M' in str(a) else a)
gplay_df['Size'] = gplay_df['Size'].apply(lambda a: str(a).replace(',', '') if ',' in str(a) else a)
gplay_df['Size'] = gplay_df['Size'].apply(lambda a: float(str(a).replace('k', ''))/1000 if 'k' in str(a) else a)
gplay_df['Size'] = gplay_df['Size'].apply(lambda a: str(a).replace('Varies with device', 'NaN') if 'Varies with device' in str(a) else a)

# Preparation for column Installs
gplay_df['Installs'] = gplay_df['Installs'].apply(lambda a: str(a).replace('+', '') if '+' in str(a) else a)
gplay_df['Installs'] = gplay_df['Installs'].apply(lambda a: str(a).replace(',', '') if ',' in str(a) else a)
gplay_df['Installs'] = gplay_df['Installs'].apply(lambda a: str(a).replace('Free', 'NaN') if 'Free' in str(a) else a)

# Preparation for column Price
gplay_df['Price'] = gplay_df['Price'].apply(lambda a: str(a).replace('$', '') if '$' in str(a) else a)
gplay_df['Price'] = gplay_df['Price'].apply(lambda a: str(a).replace('Everyone', 'NaN') if 'Everyone' in str(a) else a)

# Convert 'Reviews' column to string type
gplay_df['Reviews'] = gplay_df['Reviews'].astype(str)

# Preparation for column Reviews
gplay_df = gplay_df[~gplay_df['Reviews'].str.contains("3.0M")]

# Converting all the values of these columns to floats
gplay_df.loc[:, 'Size'] = gplay_df['Size'].apply(lambda a: float(a))
gplay_df.loc[:, 'Installs'] = gplay_df['Installs'].apply(lambda a: float(a))
gplay_df.loc[:, 'Price'] = gplay_df['Price'].apply(lambda a: float(a))
gplay_df.loc[:, 'Reviews'] = gplay_df['Reviews'].apply(lambda a: float(a))
gplay_df.loc[:, 'Rating'] = gplay_df['Rating'].apply(lambda a: float(a))

#After cleaning these columns and checking the length of the data once again:
z = len(gplay_df)
print ('The number of unique App records in the data set is: ', z)

"""## Data Visualization"""

#Data Visualisation:
no_of_distinct_categories =  len(set(gplay_df['Category']))
print ('The dataset contains Apps belonging to', no_of_distinct_categories, 'Categories')

#Visualisation using matplotlib
#Number of categories of apps in the store
x = gplay_df.Category
fig, ax = plt.subplots()
fig.set_size_inches(15, 6)
fig.autofmt_xdate()
no_of_cat_plot = sns.categorical.countplot(x)
plt.show()

#The results of the above visualisation can be better represented in a table

cat_table = gplay_df.groupby('Category').size()
cat_table = cat_table.reset_index(name='Count').nlargest(10,'Count')
cat_table

#Distribution of ratings for the dataset
sns.distplot(gplay_df['Rating'])

import seaborn as sns

#Pairplots
sns.pairplot(gplay_df)

# Selecting only numeric columns
numeric_columns = gplay_df.select_dtypes(include=['float64', 'int64'])

# Correlation Heatmap for numerical variables
sns.heatmap(numeric_columns.corr())

"""## Models for prediction of the parameter Rating"""

#Displaying the columns of the dataframe
gplay_df.columns

# Function for preprocessing
cat_columns = ['Category','Type','Content Rating', 'Android Ver']
num_columns = ['Reviews','Size','Price', 'Installs']
def preprocess(gplay_df):
  preprocess_features = pd.DataFrame()
  for i in cat_columns:
    preprocess_features[i] = gplay_df[i]
    preprocess_features[i] = preprocess_features[i].astype('category')
  preprocess_features = pd.get_dummies(preprocess_features[cat_columns], drop_first = 'True')
  for i in num_columns:
    preprocess_features[i] = gplay_df[i].astype('float')
  return preprocess_features

def preprocess_target(gplay_df):
  output_targets = gplay_df['Rating'].astype('float')
  return output_targets

#Data after preprocessing
gplay_df =gplay_df.fillna(0)
preproc_data = preprocess(gplay_df)
target = preprocess_target(gplay_df)

# Correlation for preproc_data
plt.figure(figsize=(20,20))
x = pd.concat([preproc_data, target], axis=1)
sns.heatmap(x.corr())

#Checking the shapes of the preprocessed data and the target variable 'Rating'
print(preproc_data.shape)
print(target.shape)

# Splitting the Dataset into training and testing sets
####X_train - Predictor Variables for training
####y_train - Target variable for training
####X_test - Known predictor Variables for testing
####y_test - Target variable for testing

from sklearn.model_selection import train_test_split as ts
X_train, X_test, y_train, y_test = ts(preproc_data, target, test_size=0.3, random_state=101)



"""## **Neural Network model**"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from tensorflow.python.data import Dataset
from tensorflow import keras


model = keras.Sequential([
    keras.layers.Dense(64, activation = tf.nn.relu,
                      input_shape = (preproc_data.shape[1],)),
    keras.layers.Dense(32, activation = tf.nn.relu),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(16, activation = tf.nn.relu),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(1, activation = tf.nn.relu)

])

optimizer = tf.keras.optimizers.Adam()


#The loss is the mean square error and the metric used is the mean absolute error
model.compile(loss='mse',
             optimizer = optimizer,
             metrics = ['mae'])

model.summary()

#To check whether our model runs
class Dot(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs):
    if epoch % 100 == 0 : print('')
    print ('.', end='')

set_epoch = 1000
tf.random.set_seed(1)

# Store training stats
# Convert boolean values to float
X_train = X_train.astype(float)
X_test = X_test.astype(float)
y_train = y_train.astype(float)
y_test = y_test.astype(float)

# Store training stats
fitting = model.fit(X_train, y_train, batch_size=1000, epochs=set_epoch,
                    validation_data=(X_test, y_test), verbose=0,
                    callbacks=[Dot()])

print(fitting.history)

# Plotting the model
import matplotlib.pyplot as plt

def plot_history(history):
    plt.figure()
    plt.xlabel('Epoch')
    plt.ylabel('MAE in 1000')

    # Check if 'mean_absolute_error' and 'val_mean_absolute_error' are available in history
    if 'mean_absolute_error' in history.history and 'val_mean_absolute_error' in history.history:
        plt.plot(history.epoch, np.array(history.history['mean_absolute_error']), label='Train MAE')
        plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']), label='Validation MAE')
        plt.legend()
        plt.xlim([0, 1000])
        plt.ylim([0, 4])
    else:
        print("MAE metrics are not available in the history object.")

plot_history(fitting)

model.predict(X_test)

#Saving Measurements of the model
pred = model.predict(X_test)
from sklearn import metrics
mae_nn = metrics.mean_absolute_error(y_test,pred)
mse_nn = metrics.mean_squared_error(y_test, pred)
rmse_nn = np.sqrt(metrics.mean_squared_error(y_test, pred))

print( mae_nn, mse_nn, rmse_nn)

#Decision Tree Model
from sklearn.tree import DecisionTreeRegressor
dt = DecisionTreeRegressor()
#Fitting
dt.fit(X_train,y_train)

from sklearn.tree import DecisionTreeRegressor

# Define and train your decision tree model
dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)

# Make predictions on the test data
pred1 = dt.predict(X_test)

# Calculate evaluation metrics
mae_dt = metrics.mean_absolute_error(y_test, pred1)
mse_dt = metrics.mean_squared_error(y_test, pred1)
rmse_dt = np.sqrt(mse_dt)

# Print evaluation metrics
print("Mean Absolute Error:", mae_dt)
print("Mean Squared Error:", mse_dt)
print("Root Mean Squared Error:", rmse_dt)

#light gbm : Gradient boosted method for

import lightgbm as lgb
var = np.arange(1,50,5)
#Grid Search for getting parameters

from sklearn.model_selection import GridSearchCV
gridParams = {
'learning_rate' : [0.1, 0.12, 0.14,0.16,0.18, 0.2],
    'num_leaves' : [ 20,21,22,23,24,25,26,27]
}

#model for gridsearch

mdl = lgb.LGBMRegressor(metric = 'mae',
                       objective = 'regression',
                       n_estimators= 20000,
                       bagging_fraction = 0.7,
                       num_threads = 4,
                       colsample_bytree = 0.7,
                       num_boost_round = 100)

grid = GridSearchCV(mdl, gridParams, verbose =4, n_jobs = -1, scoring = 'neg_mean_absolute_error')

grid.fit(X_train, y_train)

print(grid.best_params_)
print(grid.best_score_)

import lightgbm as lgb

param = {'learning_rate': 0.1, 'boosting_type': 'gbdt', 'num_leaves':25,
         'nthread':4, 'num_trees':100, 'objective': 'regression',
         'metric':'mse'}

import lightgbm as lgb
import os
import contextlib

# Suppress LightGBM output temporarily
with open(os.devnull, 'w') as devnull:
    with contextlib.redirect_stdout(devnull):
        train_data = lgb.Dataset(X_train, y_train)
        test_data = lgb.Dataset(X_test, y_test)

# Train your LightGBM model
lgbt = lgb.train(param, train_set=train_data, num_boost_round=100)

pred2 = lgbt.predict(X_test, num_iteration = lgbt.best_iteration)

mae_lgb = metrics.mean_absolute_error(y_test,pred2)
mse_lgb = metrics.mean_squared_error(y_test, pred2)
rmse_lgb = np.sqrt(metrics.mean_squared_error(y_test, pred2))

print( mae_lgb, mse_lgb, rmse_lgb)

#visualizing important predictors
import matplotlib.pyplot as plt

lgb.plot_importance(lgbt, max_num_features = 10)

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X_train,y_train)

pred3 = lr.predict(X_test)

mae_lr = metrics.mean_absolute_error(y_test,pred3)
mse_lr = metrics.mean_squared_error(y_test, pred3)
rmse_lr = np.sqrt(metrics.mean_squared_error(y_test, pred3))

print( mae_lr, mse_lr, rmse_lr)

a = np.array([mae_lr,mse_lr,rmse_lr])
b = np.array([mae_nn,mse_nn,rmse_nn])
c = np.array([mae_dt,mse_dt,rmse_dt])
d = np.array([mae_lgb,mse_lgb,rmse_lgb])

x = pd.DataFrame(data =[a,b,c,d], columns = ['MAE','MSE','RMSE'], index = ['Multiple Linear Regression', 'Neural Networks', 'Decision Tree Regression', 'Light Gradient Boosted Model'])
x

